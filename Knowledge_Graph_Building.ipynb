{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b54f46c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5d7d59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_path = \"C:/Program Files/Java/jdk-18.0.1.1/bin/java.exe\"\n",
    "os.environ[\"JAVAHOME\"] = java_path\n",
    "\n",
    "jar = \"C:/Users/Public/utility/stanford-postagger-full-2020-11-17/stanford-postagger.jar\"\n",
    "model = \"C:/Users/Public/utility/stanford-postagger-full-2020-11-17/models/english-bidirectional-distsim.tagger\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9623576",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tagger = StanfordPOSTagger(model, jar, encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9efbeb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              sentences  labels\n",
      "0                     His face became as black as coal.       1\n",
      "1     My uncle is as blind as a bat without his spec...       1\n",
      "2                      Naina was as cool as a cucumber.       1\n",
      "3                   The soldier was as brave as a lion.       1\n",
      "4                             He is cunning like a fox.       1\n",
      "...                                                 ...     ...\n",
      "1810         They like to explore new cities and towns.       0\n",
      "1811       They like to go on hiking and camping trips.       0\n",
      "1812  They like to volunteer at local charities and ...       0\n",
      "1813  They like to watch TV series and binge-watch s...       0\n",
      "1814  They like to spend time with their families an...       0\n",
      "\n",
      "[1815 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pandas.read_csv('C:/Users/sanmeet/Documents/4th_Year/4th_Year_Sem2/BTech_Project/dataset/dataset.csv',encoding=\"ISO-8859-1\",header=[0])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa3d23e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9aa363d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'over', 'empty', 'it', 'through', 'thereby', 'nobody', 'seemed', 'give', 'upon', 'whenever', 'being', 'latter', '‘m', 'put', 'could', 'they', 'toward', 'everything', 'you', 'the', 'become', 'will', 'really', 'any', 'alone', 'otherwise', 'per', 'everywhere', 'still', 'too', 'i', 'yourself', 'therein', 'serious', 'than', 'nor', 'anyway', 'often', 'much', 'formerly', 'former', '‘re', 'either', 'hereby', 'full', 'whereafter', '’ll', 'myself', 'besides', 'where', 'might', 'however', 'anything', 'their', 'at', 'twelve', 'moreover', 'himself', 'thereupon', 'though', 'own', 'but', 'becomes', 'forty', 'ever', 'everyone', 'see', 'elsewhere', 'whole', 'two', '’m', '‘s', \"n't\", 'off', 'becoming', 'made', 'now', 'therefore', 'whereupon', 'may', 'hence', 'so', 'such', 'would', 'quite', 'whence', 'call', 'once', 'me', 'without', 'some', 'n’t', 'also', 'how', 'n‘t', 'anyhow', 'against', 'amount', 'from', 'else', 'during', 'anywhere', 'that', 'is', 'whereby', 'both', 'them', 'were', 'he', 'neither', 'her', 'be', 'every', 'hereupon', 'wherein', 'then', 'within', 'nevertheless', 'make', 'sixty', 'most', 'behind', \"'s\", 'about', 'always', 'on', 'again', 'yet', 'six', 'say', 'yourselves', 'above', '‘d', 'indeed', 'ca', 'its', 'thereafter', 'we', 'something', \"'ve\", 'became', 'perhaps', 'here', 'many', 'whom', 'less', 'front', 'go', 'under', 'his', 'several', 'themselves', 'sometime', 'keep', 'fifty', \"'re\", 'what', 'until', 'to', 'least', 'had', 'as', 'does', 'beside', 'someone', '’ve', 'show', 'when', 'thence', 'have', '’s', 'down', 'all', 'my', 'him', 'across', 'three', 'due', 'your', 'must', 'whoever', 'other', 'never', 'should', 'well', 'thru', 'by', 'nine', 'onto', 'wherever', \"'d\", 'seem', 'somehow', 'or', 'below', 'thus', 'even', 'except', 'same', 'very', 'regarding', 'eight', 'beyond', 'why', 'in', 'further', 'various', 'after', 'done', 'next', 'no', 'if', 'am', 're', 'side', 'namely', 'which', 'whether', 'been', 'just', 'nothing', 'mostly', 'seems', 'cannot', 'are', 'up', 'mine', 'please', 'herself', 'and', 'only', 'there', '‘ve', 'although', 'back', 'hers', 'she', 'latterly', 'eleven', 'those', 'these', 'with', 'among', 'fifteen', 'of', 'unless', 'last', 'while', 'ourselves', 'another', 'ten', 'towards', 'whatever', 'a', 'top', 'do', '’re', 'ours', 'was', 'using', 'whither', 'almost', 'part', 'out', 'few', 'between', 'this', 'hereafter', 'our', 'somewhere', 'via', 'already', 'amongst', 'has', 'used', 'whereas', 'more', 'anyone', 'who', 'because', 'whose', '’d', 'can', 'afterwards', 'itself', 'noone', 'around', 'an', 'together', 'move', 'rather', 'into', 'take', 'doing', 'nowhere', 'none', 'seeming', \"'m\", 'twenty', 'one', 'beforehand', 'hundred', 'since', 'not', 'four', 'sometimes', 'for', 'each', 'along', 'get', 'bottom', 'before', 'others', 'enough', 'herein', 'us', 'did', 'yours', 'third', '‘ll', \"'ll\", 'meanwhile', 'name', 'five', 'throughout', 'first'}\n"
     ]
    }
   ],
   "source": [
    "all_stopwords = nlp.Defaults.stop_words\n",
    "print(all_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "866073ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_sentence = nlp(df.loc[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e85e15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[His face, coal]\n"
     ]
    }
   ],
   "source": [
    "chunks = list(nlp_sentence.noun_chunks)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a103841",
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = ['NN', 'NNP', 'NNS', 'NNPS']\n",
    "pronouns = ['PRP', 'PRP$']\n",
    "adjectives = ['JJ', 'JJR', 'JJS']\n",
    "verbs = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "helping_verbs = ['be', 'can', 'could', 'dare', 'do', 'have', 'may', 'might', 'must', 'need', 'ought', 'shall', 'should', 'will', 'would']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a45681f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get noun chunks\n",
    "def get_noun_phrase(sentence):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    nlp_sentence = nlp(sentence)\n",
    "    chunks = list(nlp_sentence.noun_chunks)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8ca6d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', 'PRP')]\n",
      "\n",
      "\n",
      "\n",
      "music\n"
     ]
    }
   ],
   "source": [
    "target = \"\"\n",
    "attribute = \"\"\n",
    "source = \"\"\n",
    "\n",
    "for i in range(1):\n",
    "    # for metaphorical sentences\n",
    "    if df.loc[i]['labels'] == 1:\n",
    "#         sentence = df.loc[i]['sentences'].lower()\n",
    "        sentence = \"I like listening to music as I do the ironing.\".lower()\n",
    "        \n",
    "        # for simile sentences having 'as' as a comparator\n",
    "        if sentence.count(' as ') == 2:\n",
    "            start = sentence.find(' as ')\n",
    "            end = sentence.rfind(' as ')\n",
    "            \n",
    "            # +4 for skipping ' as '\n",
    "            attribute = sentence[start+4:end].strip()\n",
    "            \n",
    "            target_noun_phrases = get_noun_phrase(sentence[:start])\n",
    "            source_noun_phrases = get_noun_phrase(sentence[end+4:])\n",
    "            print(target_noun_phrases)\n",
    "\n",
    "            if(len(target_noun_phrases) == 0):\n",
    "                words = nltk.word_tokenize(sentence[:start])\n",
    "                tagged_words = pos_tagger.tag(words)\n",
    "                for each in tagged_words:\n",
    "                    if each[1] in nouns or each[1] in pronouns:\n",
    "                        target = each[0]\n",
    "            else:\n",
    "                target_noun_phrases_lst = word_tokenize(str(target_noun_phrases[0]).strip())\n",
    "                print(\"ddd\", target_noun_phrases_lst)\n",
    "                if len(target_noun_phrases_lst) > 1:\n",
    "                    target_noun_phrases_lst_without_sw = [word for word in target_noun_phrases_lst if not word in all_stopwords]\n",
    "                    print(target_noun_phrases_lst_without_sw)\n",
    "                    target = ' '.join(target_noun_phrases_lst_without_sw)\n",
    "                else:\n",
    "                    target = target_noun_phrases_lst[0]\n",
    "            print(source_noun_phrases)\n",
    "            if(len(source_noun_phrases) == 0):\n",
    "                words = nltk.word_tokenize(sentence[end+4:])\n",
    "                tagged_words = pos_tagger.tag(words)\n",
    "                print(tagged_words)\n",
    "                for each in tagged_words:\n",
    "                    if each[1] in nouns or each[1] in pronouns:\n",
    "                        source = each[0]\n",
    "            else:\n",
    "                source_noun_phrases_lst = word_tokenize(str(source_noun_phrases[0]).strip())\n",
    "                print(source_noun_phrases_lst)\n",
    "                if len(source_noun_phrases_lst) > 1:\n",
    "                    source_noun_phrases_lst_without_sw = [word for word in source_noun_phrases_lst if not word in all_stopwords]\n",
    "                    source = ' '.join(source_noun_phrases_lst_without_sw)\n",
    "                else:\n",
    "                    source = source_noun_phrases_lst[0]\n",
    "                \n",
    "        # for simile sentences having 'like' as a comparator\n",
    "        if sentence.count(' like ') == 1:\n",
    "            like_index = sentence.find(' like ')\n",
    "            before_like = sentence[:like_index]\n",
    "            after_like = sentence[like_index+6:]\n",
    "\n",
    "            words = nltk.word_tokenize(before_like)\n",
    "            tagged_words = pos_tagger.tag(words)\n",
    "            \n",
    "            print(tagged_words)\n",
    "\n",
    "            for each in tagged_words:\n",
    "                if each[1] in adjectives:\n",
    "                    attribute = each[0]\n",
    "                elif each[1] in verbs:\n",
    "                    present_form = WordNetLemmatizer().lemmatize(each[0],'v')\n",
    "                    if present_form not in helping_verbs:\n",
    "                        attribute = present_form\n",
    "\n",
    "            attribute_index = before_like.find(attribute)\n",
    "            target_noun_phrases = get_noun_phrase(before_like[:attribute_index])\n",
    "            source_noun_phrases = get_noun_phrase(after_like)\n",
    "\n",
    "            if(len(target_noun_phrases) == 0):\n",
    "                words = nltk.word_tokenize(before_like[:attribute_index])\n",
    "                tagged_words = pos_tagger.tag(words)\n",
    "                for each in tagged_words:\n",
    "                    if each[1] in nouns or each[1] in pronouns:\n",
    "                        target = each[0]\n",
    "            else:\n",
    "                target_noun_phrases_lst = word_tokenize(str(target_noun_phrases[0]).strip())\n",
    "                if len(target_noun_phrases_lst) > 1:\n",
    "                    target_noun_phrases_lst_without_sw = [word for word in target_noun_phrases_lst if not word in all_stopwords]\n",
    "                    target = ' '.join(target_noun_phrases_lst_without_sw)\n",
    "                else:\n",
    "                    target = target_noun_phrases_lst[0]\n",
    "\n",
    "            if(len(source_noun_phrases) == 0):\n",
    "                words = nltk.word_tokenize(after_like)\n",
    "                tagged_words = pos_tagger.tag(words)\n",
    "                for each in tagged_words:\n",
    "                    if each[1] in nouns or each[1] in pronouns:\n",
    "                        source = each[0]\n",
    "            else:\n",
    "                source_noun_phrases_lst = word_tokenize(str(source_noun_phrases[0]).strip())\n",
    "                if len(source_noun_phrases_lst) > 1:\n",
    "                    source_noun_phrases_lst_without_sw = [word for word in source_noun_phrases_lst if not word in all_stopwords]\n",
    "                    source = ' '.join(source_noun_phrases_lst_without_sw)\n",
    "                else:\n",
    "                    source = source_noun_phrases_lst[0]\n",
    "\n",
    "        print()\n",
    "        print(target)\n",
    "        print(attribute)\n",
    "        print(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efd51b6",
   "metadata": {},
   "source": [
    "## Writing Target, Attribute and Source into excel file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2142fef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlwt\n",
    "from xlwt import Workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb4cf93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workbook is created\n",
    "wb2 = Workbook()\n",
    "\n",
    "# add_sheet is used to create sheet.\n",
    "sheet2 = wb2.add_sheet('Sheet 1', cell_overwrite_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2273ebf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying style\n",
    "style = xlwt.easyxf('font: bold 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ca1498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet2.write(0, 0, 'Target', style)\n",
    "sheet2.write(0, 1, 'Attribute', style)\n",
    "sheet2.write(0, 2, 'Source', style)\n",
    "sheet2.write(0, 3, 'labels', style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc626a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"\"\n",
    "attribute = \"\"\n",
    "source = \"\"\n",
    "row_index = 1\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    # for metaphorical sentences\n",
    "    if df.loc[i]['labels'] == 1:\n",
    "        sentence = df.loc[i]['sentences'].lower()\n",
    "        \n",
    "        # for simile sentences having 'as' as a comparator\n",
    "        if sentence.count(' as ') == 2:\n",
    "            start = sentence.find(' as ')\n",
    "            end = sentence.rfind(' as ')\n",
    "            \n",
    "            # +4 for skipping ' as '\n",
    "            attribute = sentence[start+4:end].strip()\n",
    "            \n",
    "            target_noun_phrases = get_noun_phrase(sentence[:start])\n",
    "            source_noun_phrases = get_noun_phrase(sentence[end+4:])\n",
    "\n",
    "            if(len(target_noun_phrases) == 0):\n",
    "                words = nltk.word_tokenize(sentence[:start])\n",
    "                tagged_words = pos_tagger.tag(words)\n",
    "                for each in tagged_words:\n",
    "                    if each[1] in nouns or each[1] in pronouns:\n",
    "                        target = each[0]\n",
    "            else:\n",
    "                target_noun_phrases_lst = word_tokenize(str(target_noun_phrases[0]).strip())\n",
    "                if len(target_noun_phrases_lst) > 1:\n",
    "                    target_noun_phrases_lst_without_sw = [word for word in target_noun_phrases_lst if not word in all_stopwords]\n",
    "                    target = ' '.join(target_noun_phrases_lst_without_sw)\n",
    "                else:\n",
    "                    target = target_noun_phrases_lst[0]\n",
    "            \n",
    "            if(len(source_noun_phrases) == 0):\n",
    "                words = nltk.word_tokenize(sentence[end+4:])\n",
    "                tagged_words = pos_tagger.tag(words)\n",
    "                for each in tagged_words:\n",
    "                    if each[1] in nouns or each[1] in pronouns:\n",
    "                        source = each[0]\n",
    "            else:\n",
    "                source_noun_phrases_lst = word_tokenize(str(source_noun_phrases[0]).strip())\n",
    "                if len(source_noun_phrases_lst) > 1:\n",
    "                    source_noun_phrases_lst_without_sw = [word for word in source_noun_phrases_lst if not word in all_stopwords]\n",
    "                    source = ' '.join(source_noun_phrases_lst_without_sw)\n",
    "                else:\n",
    "                    source = source_noun_phrases_lst[0]\n",
    "                \n",
    "        # for simile sentences having 'like' as a comparator\n",
    "        if sentence.count(' like ') == 1:\n",
    "            like_index = sentence.find(' like ')\n",
    "            before_like = sentence[:like_index]\n",
    "            after_like = sentence[like_index+6:]\n",
    "\n",
    "            words = nltk.word_tokenize(before_like)\n",
    "            tagged_words = pos_tagger.tag(words)\n",
    "\n",
    "            for each in tagged_words:\n",
    "                if each[1] in adjectives:\n",
    "                    attribute = each[0]\n",
    "                elif each[1] in verbs:\n",
    "                    present_form = WordNetLemmatizer().lemmatize(each[0],'v')\n",
    "                    if present_form not in helping_verbs:\n",
    "                        attribute = present_form\n",
    "\n",
    "            attribute_index = before_like.find(attribute)\n",
    "            target_noun_phrases = get_noun_phrase(before_like[:attribute_index])\n",
    "            source_noun_phrases = get_noun_phrase(after_like)\n",
    "\n",
    "            if(len(target_noun_phrases) == 0):\n",
    "                words = nltk.word_tokenize(before_like[:attribute_index])\n",
    "                tagged_words = pos_tagger.tag(words)\n",
    "                for each in tagged_words:\n",
    "                    if each[1] in nouns or each[1] in pronouns:\n",
    "                        target = each[0]\n",
    "            else:\n",
    "                target_noun_phrases_lst = word_tokenize(str(target_noun_phrases[0]).strip())\n",
    "                if len(target_noun_phrases_lst) > 1:\n",
    "                    target_noun_phrases_lst_without_sw = [word for word in target_noun_phrases_lst if not word in all_stopwords]\n",
    "                    target = ' '.join(target_noun_phrases_lst_without_sw)\n",
    "                else:\n",
    "                    target = target_noun_phrases_lst[0]\n",
    "\n",
    "            if(len(source_noun_phrases) == 0):\n",
    "                words = nltk.word_tokenize(after_like)\n",
    "                tagged_words = pos_tagger.tag(words)\n",
    "                for each in tagged_words:\n",
    "                    if each[1] in nouns or each[1] in pronouns:\n",
    "                        source = each[0]\n",
    "            else:\n",
    "                source_noun_phrases_lst = word_tokenize(str(source_noun_phrases[0]).strip())\n",
    "                if len(source_noun_phrases_lst) > 1:\n",
    "                    source_noun_phrases_lst_without_sw = [word for word in source_noun_phrases_lst if not word in all_stopwords]\n",
    "                    source = ' '.join(source_noun_phrases_lst_without_sw)\n",
    "                else:\n",
    "                    source = source_noun_phrases_lst[0]\n",
    "\n",
    "#         print()\n",
    "#         print(target)\n",
    "#         print(attribute)\n",
    "#         print(source)\n",
    "        sheet2.write(row_index, 0, target)\n",
    "        sheet2.write(row_index, 1, attribute)\n",
    "        sheet2.write(row_index, 2, source)\n",
    "        sheet2.write(row_index, 3, 1)\n",
    "        row_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc4a56ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "wb2.save(\"knowledge_graph_triples.xls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb67fdc",
   "metadata": {},
   "source": [
    "## Command for creating Knowledge Graph (Neo4j) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e575b8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Target      Attribute    Source\n",
      "0          face          black      coal\n",
      "1         uncle          blind       bat\n",
      "2         naina           cool  cucumber\n",
      "3       soldier          brave      lion\n",
      "4            he        cunning       fox\n",
      "...         ...            ...       ...\n",
      "1553       love  unconditional    warmth\n",
      "1554    purring       soothing   lullaby\n",
      "1555  small nut           hard      rock\n",
      "1556  nut shell          tough   leather\n",
      "1557        nut           rich    butter\n",
      "\n",
      "[1558 rows x 3 columns]\n",
      "(1558, 3)\n"
     ]
    }
   ],
   "source": [
    "# read by default 1st sheet of an excel file\n",
    "d_frame = pandas.read_excel('knowledge_graph_triples.xls')\n",
    "\n",
    "print(d_frame)\n",
    "print(d_frame.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b727d466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Target      Attribute    Source\n",
      "0          face          black      coal\n",
      "1         uncle          blind       bat\n",
      "2         naina           cool  cucumber\n",
      "3       soldier          brave      lion\n",
      "4            he        cunning       fox\n",
      "...         ...            ...       ...\n",
      "1526       love  unconditional    warmth\n",
      "1527    purring       soothing   lullaby\n",
      "1528  small nut           hard      rock\n",
      "1529  nut shell          tough   leather\n",
      "1530        nut           rich    butter\n",
      "\n",
      "[1531 rows x 3 columns]\n",
      "(1531, 3)\n"
     ]
    }
   ],
   "source": [
    "# dropping duplicate rows from dataframe\n",
    "d_frame = d_frame.drop_duplicates(ignore_index=True)\n",
    "\n",
    "print(d_frame)\n",
    "print(d_frame.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d0c2ff05",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"kb.txt\", \"w\") as f:\n",
    "    f.write('CREATE' + '\\n')\n",
    "    target_set = set()\n",
    "    source_set = set()\n",
    "    for i in range(d_frame.shape[0]):\n",
    "        target_value = \"\"\n",
    "        target_var = \"\"\n",
    "        attribute_value = \"\"\n",
    "        attribute_var = \"\"\n",
    "        source_value = \"\"\n",
    "        source_var = \"\"\n",
    "        \n",
    "        target_value = d_frame.loc[i][0]\n",
    "        attribute_value = d_frame.loc[i][1]\n",
    "        source_value = d_frame.loc[i][2]\n",
    "        \n",
    "        # '-' is to be replaced by ' ' as it is not allowed in variable name in Neo4j\n",
    "        if target_value.find('-') != -1:\n",
    "            target_value = target_value.replace('-', ' ')\n",
    "        if attribute_value.find('-') != -1:\n",
    "            attribute_value = attribute_value.replace('-', ' ')\n",
    "        if source_value.find('-') != -1:\n",
    "            source_value = source_value.replace('-', ' ')\n",
    "\n",
    "        # \"'\" is to be replaced by \"\" as it is not allowed in variable name in Neo4j\n",
    "        if target_value.find(\"'\"):\n",
    "            target_value = target_value.replace(\"'\", \"\")\n",
    "        if attribute_value.find(\"'\"):\n",
    "            attribute_value = attribute_value.replace(\"'\", \"\")\n",
    "        if attribute_value.find(\"'\"):\n",
    "            attribute_value = attribute_value.replace(\"'\", \"\")\n",
    "\n",
    "        target_var = '_'.join((target_value).split())\n",
    "        attribute_var = '_'.join((attribute_value).split())\n",
    "        source_var = '_'.join((source_value).split())\n",
    "        \n",
    "        target_var = target_var + \"_t\"\n",
    "        if target_var not in target_set:\n",
    "            f.write('(' + target_var + ':TARGET{value:\"' + target_value + '\"}),' + '\\n')\n",
    "            target_set.add(target_var)\n",
    "        source_var = source_var + \"_s\"\n",
    "        if source_var not in source_set:\n",
    "            f.write('(' + source_var + ':SOURCE{value:\"' + source_value + '\"}),' + '\\n')\n",
    "            source_set.add(source_var)\n",
    "        \n",
    "\n",
    "        if i != d_frame.shape[0]-1:\n",
    "            f.write('(' + source_var + ')-[:ATTRIBUTE {value:\"' + attribute_value + '\"}]->('+ target_var + '),' + '\\n')\n",
    "        else:\n",
    "            f.write('(' + source_var + ')-[:ATTRIBUTE {value:\"' + attribute_value + '\"}]->('+ target_var + ');' + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96ba1d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_path = \"C:/Program Files/Java/jdk-18.0.1.1/bin/java.exe\"\n",
    "os.environ[\"JAVAHOME\"] = java_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "895e546d",
   "metadata": {},
   "outputs": [],
   "source": [
    "jar = \"C:/Users/Public/utility/stanford-postagger-full-2020-11-17/stanford-postagger.jar\"\n",
    "model = \"C:/Users/Public/utility/stanford-postagger-full-2020-11-17/models/english-bidirectional-distsim.tagger\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aafd68ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tagger = StanfordPOSTagger(model, jar, encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2f53e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"kb3.txt\", \"w\") as f:\n",
    "    f.write('CREATE' + '\\n')\n",
    "    target_set = set()\n",
    "    source_set = set()\n",
    "    for i in range(d_frame.shape[0]):\n",
    "        target_value = \"\"\n",
    "        target_var = \"\"\n",
    "        attribute_value = \"\"\n",
    "        attribute_var = \"\"\n",
    "        source_value = \"\"\n",
    "        source_var = \"\"\n",
    "        \n",
    "        target_value = d_frame.loc[i][0]\n",
    "        attribute_value = d_frame.loc[i][1]\n",
    "        source_value = d_frame.loc[i][2]\n",
    "        \n",
    "        # '-' is to be replaced by ' ' as it is not allowed in variable name in Neo4j\n",
    "        if target_value.find('-') != -1:\n",
    "            target_value = target_value.replace('-', ' ')\n",
    "        if attribute_value.find('-') != -1:\n",
    "            attribute_value = attribute_value.replace('-', ' ')\n",
    "        if source_value.find('-') != -1:\n",
    "            source_value = source_value.replace('-', ' ')\n",
    "\n",
    "        # \"'\" is to be replaced by \"\" as it is not allowed in variable name in Neo4j\n",
    "        if target_value.find(\"'\"):\n",
    "            target_value = target_value.replace(\"'\", \"\")\n",
    "        if attribute_value.find(\"'\"):\n",
    "            attribute_value = attribute_value.replace(\"'\", \"\")\n",
    "        if attribute_value.find(\"'\"):\n",
    "            attribute_value = attribute_value.replace(\"'\", \"\")\n",
    "            \n",
    "        # Stanford POS tagging\n",
    "        words = nltk.word_tokenize(target_value)\n",
    "        target_tagged_words = pos_tagger.tag(words)\n",
    "        \n",
    "        words = nltk.word_tokenize(source_value)\n",
    "        source_tagged_words = pos_tagger.tag(words)\n",
    "        \n",
    "        words = nltk.word_tokenize(attribute_value)\n",
    "        attribute_tagged_words = pos_tagger.tag(words)\n",
    "\n",
    "        target_var = '_'.join((target_value).split())\n",
    "        attribute_var = '_'.join((attribute_value).split())\n",
    "        source_var = '_'.join((source_value).split())\n",
    "        \n",
    "        target_var = target_var + \"_t\"\n",
    "        if target_var not in target_set:\n",
    "            f.write('(' + target_var + ':TARGET{value:\"' + target_value + '\",pos_tag:\"' + target_tagged_words[0][-1] + '\"}),' + '\\n')\n",
    "            target_set.add(target_var)\n",
    "        source_var = source_var + \"_s\"\n",
    "        if source_var not in source_set:\n",
    "            f.write('(' + source_var + ':SOURCE{value:\"' + source_value + '\",pos_tag:\"' + source_tagged_words[0][-1] + '\"}),' + '\\n')\n",
    "            source_set.add(source_var)\n",
    "        \n",
    "\n",
    "        if i != d_frame.shape[0]-1:\n",
    "            f.write('(' + source_var + ')-[:ATTRIBUTE {value:\"' + attribute_value + '\",pos_tag:\"' + attribute_tagged_words[0][1] +  '\"}]->('+ target_var + '),' + '\\n')\n",
    "        else:\n",
    "            f.write('(' + source_var + ')-[:ATTRIBUTE {value:\"' + attribute_value + '\",pos_tag:\"' + attribute_tagged_words[0][1] + '\"}]->('+ target_var + ');' + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
